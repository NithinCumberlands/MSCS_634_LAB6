# MSCS_634_LAB6
# Association Rule Mining Lab

## Overview

This repository contains the work for an Association Rule Mining lab, where I applied the **Apriori** and **FP-Growth** algorithms to mine frequent itemsets and generate association rules from a **BookCrossingThemes** dataset. The primary goal of this lab was to uncover hidden patterns in data, particularly focusing on the relationships between different book themes (genres).

## Dataset

The dataset used in this lab is the **BookCrossingThemes.csv**, which contains various attributes related to books, such as:
- **Book-Title**
- **Book-Author**
- **User-ID**
- **ISBN**
- **Book-Rating**
- **Theme** (the primary column of interest, representing the genre of each book)

For the purpose of association rule mining, I focused primarily on the **Theme** column, which was transformed into a one-hot encoded format. This allowed the themes to be treated as binary attributes for mining purposes.

## Lab Steps

### Step 1: Data Preparation
- Loaded the dataset using **pandas**.
- Cleaned the dataset and handled missing values.
- One-hot encoded the **Theme** column to prepare the data for association rule mining.

### Step 2: Frequent Itemset Mining Using Apriori
- Applied the **Apriori** algorithm to mine frequent itemsets with a minimum support threshold of 0.01 (1%).
- Visualized the top 10 frequent itemsets using **Seaborn**.

### Step 3: Frequent Itemset Mining Using FP-Growth
- Applied the **FP-Growth** algorithm, which is more efficient compared to Apriori, especially with larger datasets.
- Compared the frequent itemsets generated by FP-Growth with those obtained from Apriori.

### Step 4: Generating Association Rules
- Used the frequent itemsets to generate association rules based on the **lift** metric.
- Adjusted the minimum **lift** threshold to 0.5 to allow more rules to be generated.
- Visualized the confidence vs. lift of the association rules using a **scatter plot**.

### Step 5: Comparative Analysis
- Compared the performance of the Apriori and FP-Growth algorithms.
- Analyzed the speed and efficiency of FP-Growth, highlighting its scalability for larger datasets.

## Visualizations

1. **Top 10 Frequent Itemsets**: A bar plot that shows the most frequent themes in the dataset, helping to identify popular book genres.
2. **Confidence vs Lift of Association Rules**: A scatter plot that illustrates the relationship between the confidence and lift of generated association rules, highlighting the strength of these rules.

## Challenges and Lessons Learned
- **Threshold Selection**: One challenge was selecting appropriate thresholds for support and lift. Setting thresholds too high resulted in too few rules, while lowering them allowed more rules to be generated.
- **Interpretation of Rules**: Some rules were intuitive, but others required more analysis to understand their real-world implications.

## Technologies Used
- **Python**
- **pandas**: For data manipulation and cleaning.
- **mlxtend**: For implementing the Apriori and FP-Growth algorithms.
- **Seaborn** and **Matplotlib**: For data visualization.

## Files in the Repository
1. **Association_Rule_Mining_Lab.ipynb**: The Jupyter notebook containing the full code and analysis.
2. **BookCrossingThemes.csv**: The dataset used for the lab (Make sure this file is placed correctly in your local or online repository).
3. **README.md**: This file, providing an overview of the lab and instructions.

## How to Run

1. Clone the repository:
